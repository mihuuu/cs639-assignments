# Assignment 2: BPE Tokenizer Training  
**CS 639: Deep Learning for NLP**

This assignment focuses on implementing a **byte-level Byte-Pair Encoding (BPE) tokenizer**, a core component in modern NLP systems. You will complete missing parts of a tokenizer training pipeline and train a tokenizer on real text data.

## Setup

### Environment
We manage our environments with `uv` to ensure reproducibility, portability, and ease of use.
Install `uv` [here](https://github.com/astral-sh/uv) (recommended), or run `pip install uv`/`brew install uv`.
We recommend reading a bit about managing projects in `uv` [here](https://docs.astral.sh/uv/guides/projects/#managing-dependencies) (you will not regret it!).

You can now run any code in the repo using
```sh
uv run <python_file_path>
```
and the environment will be automatically solved and activated when necessary.

### Download data
Download the TinyStories data

``` sh
mkdir -p data
cd data

wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

cd ..
```







## Files

### `tokenizer_hw.py` (main file)

This file contains the core implementation of a Byte-Pair Encoding (BPE) tokenizer.
You will implement the tokenizer training pipeline, including pre-tokenization, pair counting, and iterative merge updates.

You must implement the following functions:

- `build_split_expr()` — Construct a regex-based splitting expression that correctly separates text while preserving any provided special tokens (e.g., `pad`, `unk`, etc.). Special tokens must remain intact during tokenization.
- `pretokenize_text()` — Perform regex-based pre-tokenization and convert resulting tokens into byte-level representations. This step prepares raw text for BPE training while ensuring consistent handling of whitespace, punctuation, and Unicode characters.
- `process_chunk()` — Implement chunk-based preprocessing to allow efficient processing of large text files. This function should support scalable training by avoiding loading the entire dataset into memory at once.
- `count_pairs()` — Count frequencies of adjacent token pairs. This is the key statistical step in BPE training used to determine which merge operation should be applied next. 
- `merge_pair()` — Apply a selected BPE merge operation to token sequences and update counts accordingly. This function should be implemented carefully to ensure correctness and efficiency. 

**Do not change function signatures.**

The grading scripts will call these functions directly.


## Running Training

Train the tokenizer with:

```bash
uv run tokenizer_hw.py
```
This command will:

1. Load the training text data
2. Perform pre-tokenization
3. Iteratively learn BPE merge rules
4. Save the resulting tokenizer artifacts

Training should complete within a few minutes on a standard CPU machine (exact runtime depends on dataset size and implementation efficiency).


Outputs will be saved to:

```
tokenizer_results/
  *_vocab.pkl
  *_merges.pkl
```

Training should complete within a few minutes on CPU.



## Allowed Libraries

**Allowed:**
- Python standard library  
- `regex`  
- `numpy` (optional)

**Not allowed:**
- HuggingFace tokenizers  
- SentencePiece  
- Any external tokenizer implementations  

You must implement BPE yourself.



## Submission Format

Submit a zip file structured as:

```text
CAMPUSID/
  tokenizer_hw.py
  tokenizer_results/
    *_vocab.pkl
    *_merges.pkl
  run.sh
```

Where:

- run.sh should reproduce your training results with a single command
- Output files must match those generated by your implementation
- Do not include large datasets in the submission


## Grading Overview

Score | Criteria
---|---
90–100 | Correct implementation; tokenizer trains successfully and produces valid outputs
85–89 | Mostly correct implementation with minor correctness or performance issues
80–84 | Partial implementation; some required functionality missing
<80 | Major missing components or code does not run successfully


Efficiency, correctness, and reproducibility will all be considered.
